<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.14.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="主要写一些练习kaggle的心得和流程，主要参考Kaggle 入门指南 - 知乎 (zhihu.com) 现在发现Introduction | Kaggle也写得很好，起码完整流程是有的。">
<meta property="og:type" content="article">
<meta property="og:title" content="kaggle">
<meta property="og:url" content="http://example.com/2023/01/03/kaggle/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="主要写一些练习kaggle的心得和流程，主要参考Kaggle 入门指南 - 知乎 (zhihu.com) 现在发现Introduction | Kaggle也写得很好，起码完整流程是有的。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-01-03T06:56:30.000Z">
<meta property="article:modified_time" content="2023-06-03T12:28:25.120Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="kaggle">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2023/01/03/kaggle/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/01/03/kaggle/","path":"2023/01/03/kaggle/","title":"kaggle"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>kaggle | Hexo</title>
  






  <script async defer data-website-id="" src=""></script>

  <script defer data-domain="" src=""></script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#kaggle-%E6%AF%94%E8%B5%9B%E6%B5%81%E7%A8%8B"><span class="nav-number">1.</span> <span class="nav-text">kaggle 比赛流程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Exploratory-Data-Analysis"><span class="nav-number">1.1.</span> <span class="nav-text">Exploratory Data Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">1.1.1.</span> <span class="nav-text">可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E6%A3%80%E9%AA%8C"><span class="nav-number">1.1.2.</span> <span class="nav-text">统计检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%88%E5%8F%82%E8%80%83kaggle%E8%AF%BE%E7%A8%8BLearn-Data-Cleaning-Tutorials-kaggle-com-%EF%BC%89%EF%BC%88%E8%BF%99%E9%83%A8%E5%88%86%E5%9C%A8%E5%AE%9E%E9%99%85%E4%B8%AD%E7%94%A8%E5%88%B0%E4%BA%86%E5%8D%B3%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E7%99%BE%E5%BA%A6%E7%A8%8B%E5%BA%8F%E6%88%96%E8%80%85%E7%BF%BBKaggle%E8%AF%BE%E7%A8%8B%E9%87%8C%E9%9D%A2%E4%BB%A3%E7%A0%81%EF%BC%89"><span class="nav-number">1.1.3.</span> <span class="nav-text">数据清洗（参考kaggle课程Learn Data Cleaning Tutorials (kaggle.com)）（这部分在实际中用到了即可以直接百度程序或者翻Kaggle课程里面代码）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.1.4.</span> <span class="nav-text">数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">缺失值处理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%EF%BC%88%E5%8F%82%E8%80%83Learn-Feature-Engineering-Tutorials-kaggle-com-%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">特征工程（参考Learn Feature Engineering Tutorials (kaggle.com)）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="nav-number">1.2.0.1.</span> <span class="nav-text">什么是特征工程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E6%95%88%E7%94%A8%E6%8C%87%E6%A0%87"><span class="nav-number">1.2.1.</span> <span class="nav-text">特征效用指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E7%8E%B0%E6%96%B0%E7%9A%84%E7%89%B9%E5%BE%81"><span class="nav-number">1.2.2.</span> <span class="nav-text">发现新的特征</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Mathematical-Transforms%EF%BC%9A%E5%AF%B9%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%AE%80%E5%8D%95%E7%9A%84%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E7%BB%84%E5%90%88%EF%BC%8C%E4%B8%80%E8%88%AC%E6%9D%A5%E8%AF%B4%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E6%9D%A5%E8%B0%83%E6%95%B4%E7%BB%84%E5%90%88"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Mathematical Transforms：对特征进行简单的加减乘除组合，一般来说通过数据可视化来调整组合</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Counts-Features-describing-the-presence-or-absence-of-something-often-come-in-sets-the-set-of-risk-factors-for-a-disease-say-You-can-aggregate-such-features-by-creating-a-count"><span class="nav-number">1.2.3.</span> <span class="nav-text">Counts: Features describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a count.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-Up-and-Breaking-Down-Features-%E6%AF%94%E5%A6%82%E8%BA%AB%E4%BB%BD%E8%AF%81%E5%89%8D%E5%85%AD%E4%BD%8D%E6%98%AF%E5%9C%B0%E5%9D%80%E4%BB%80%E4%B9%88%E7%9A%84"><span class="nav-number">1.2.4.</span> <span class="nav-text">Building-Up and Breaking-Down Features: 比如身份证前六位是地址什么的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Group-Transformation-Using-an-aggregation-function-a-group-transform-combines-two-features-a-categorical-feature-that-provides-the-grouping-and-another-feature-whose-values-you-wish-to-aggregate"><span class="nav-number">1.2.5.</span> <span class="nav-text">Group Transformation: Using an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%86%E8%81%9A%E7%B1%BB%E6%A0%87%E7%AD%BE%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81"><span class="nav-number">1.2.6.</span> <span class="nav-text">将聚类标签作为特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PCA"><span class="nav-number">1.2.7.</span> <span class="nav-text">PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Target-Encoding%EF%BC%88%E9%92%88%E5%AF%B9%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F%EF%BC%89"><span class="nav-number">1.2.8.</span> <span class="nav-text">Target Encoding（针对分类变量）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Smoothing"><span class="nav-number">2.</span> <span class="nav-text">Smoothing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Pipeline"><span class="nav-number">2.1.</span> <span class="nav-text">Pipeline</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/03/kaggle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="kaggle | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kaggle
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-03 14:56:30" itemprop="dateCreated datePublished" datetime="2023-01-03T14:56:30+08:00">2023-01-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-06-03 20:28:25" itemprop="dateModified" datetime="2023-06-03T20:28:25+08:00">2023-06-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>主要写一些练习kaggle的心得和流程，主要参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25742261">Kaggle 入门指南 - 知乎 (zhihu.com)</a></p>
<p>现在发现<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/alexisbcook/introduction">Introduction | Kaggle</a>也写得很好，起码完整流程是有的。</p>
<span id="more"></span>
<p>一个简单的目录:</p>
<p>数据处理：</p>
<p>缺失值处理：1扔掉那些有缺失值的列2简单的填充imputation</p>
<p>分类变量处理：1ordinal encoding 2 onehot encoding</p>
<p>特征筛选：</p>
<p>模型选择：random forest，XGBoost</p>
<p>pipeline:</p>
<p>交叉验证：</p>
<h1 id="kaggle-比赛流程"><a href="#kaggle-比赛流程" class="headerlink" title="kaggle 比赛流程"></a>kaggle 比赛流程</h1><h2 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a><strong>Exploratory Data Analysis</strong></h2><p>常用的库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> warnings <span class="comment"># current version of seaborn generates a bunch of warnings that we&#x27;ll ignore</span></span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;white&quot;</span>, color_codes=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#解决行列显示不全</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_rows&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br></pre></td></tr></table></figure>
<p>查看数据情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#包含序号，列名，non-null，Dtype</span></span><br><span class="line">df.info()</span><br><span class="line"><span class="comment">#包含mean,std,min,max,分位数</span></span><br><span class="line">df.describe()</span><br><span class="line"><span class="comment">#前5个数据</span></span><br><span class="line">df.head()</span><br><span class="line"><span class="comment">#偏综合的，比info多一个unique value,missing value,缺失值是nan，空值是null，超级全面，但是特征太多可能显示很累</span></span><br><span class="line"><span class="comment">#可以考虑拆开用</span></span><br><span class="line"><span class="comment">#怎么感觉non-null+missing正好是数据总数呢</span></span><br><span class="line"><span class="comment">#unique value很多的时候慎用，内容太多了</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_column_details</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Column name: <span class="subst">&#123;col&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Column data type: <span class="subst">&#123;df[col].dtype&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Number of non-null values: <span class="subst">&#123;df[col].count()&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Number of unique values: <span class="subst">&#123;df[col].nunique()&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Number of count of per unique value: <span class="subst">&#123;df[col].value_counts()&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Number of missing values: <span class="subst">&#123;df[col].isnull().<span class="built_in">sum</span>()&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line">print_column_details(train)        </span><br></pre></td></tr></table></figure>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>散点图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#针对离散变量</span></span><br><span class="line">sns.countplot(data = df, x = <span class="string">&quot;col&quot;</span>);</span><br><span class="line"><span class="comment"># A seaborn jointplot shows bivariate scatterplots and univariate histograms in the same figure</span></span><br><span class="line">sns.jointplot(x=<span class="string">&quot;col1&quot;</span>, y=<span class="string">&quot;col2&quot;</span>, data=df, size=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#三变量散点图</span></span><br><span class="line"><span class="comment">#hue就是针对某一变量进行分类</span></span><br><span class="line">sns.FacetGrid(df, hue=<span class="string">&quot;col1&quot;</span>, size=<span class="number">5</span>) \</span><br><span class="line">   .<span class="built_in">map</span>(plt.scatter, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>) \</span><br><span class="line">   .add_legend()</span><br></pre></td></tr></table></figure>
<p>箱型图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#普通版</span></span><br><span class="line">sns.boxplot(x=<span class="string">&quot;col1&quot;</span>, y=<span class="string">&quot;col2&quot;</span>, data=df)</span><br><span class="line"><span class="comment">#进阶版</span></span><br><span class="line">ax = sns.boxplot(x=<span class="string">&quot;col1&quot;</span>, y=<span class="string">&quot;col2&quot;</span>, data=df)</span><br><span class="line">ax = sns.stripplot(x=<span class="string">&quot;col1&quot;</span>, y=<span class="string">&quot;col2&quot;</span>, data=df, jitter=<span class="literal">True</span>, edgecolor=<span class="string">&quot;gray&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>核密度图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#单变量的</span></span><br><span class="line">sns.kdeplot(data = df, x = <span class="string">&quot;col&quot;</span>,fill = <span class="literal">True</span>, label = <span class="string">&quot;col&quot;</span>)</span><br><span class="line">sns.distplot(df[<span class="string">&quot;col&quot;</span>])</span><br><span class="line"><span class="comment">#kde就是核密度，size作用暂时没找到</span></span><br><span class="line">sns.FacetGrid(df, hue=<span class="string">&quot;col1&quot;</span>, size=<span class="number">6</span>) \</span><br><span class="line">   .<span class="built_in">map</span>(sns.kdeplot, <span class="string">&quot;col2&quot;</span>) \</span><br><span class="line">   .add_legend()</span><br></pre></td></tr></table></figure>
<p>大杂烩</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#所有特征散点图</span></span><br><span class="line">sns.pairplot(df.drop(<span class="string">&quot;Id&quot;</span>, axis=<span class="number">1</span>), hue=<span class="string">&quot;col&quot;</span>, size=<span class="number">3</span>)</span><br><span class="line"><span class="comment">#选择对角图的类型</span></span><br><span class="line">sns.pairplot(df.drop(<span class="string">&quot;Id&quot;</span>, axis=<span class="number">1</span>), hue=<span class="string">&quot;col&quot;</span>, size=<span class="number">3</span>, diag_kind=<span class="string">&quot;kde&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>其他（一些很炫酷的图）也是针对所有特征，所以特征多了可能效果会很糟糕</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parallel coordinates plots each feature on a separate column &amp; then draws lines</span></span><br><span class="line"><span class="comment"># connecting the features for each data sample</span></span><br><span class="line"><span class="keyword">from</span> pandas.tools.plotting <span class="keyword">import</span> parallel_coordinates</span><br><span class="line">parallel_coordinates(df.drop(<span class="string">&quot;Id&quot;</span>, axis=<span class="number">1</span>), <span class="string">&quot;col&quot;</span>)</span><br><span class="line"><span class="comment"># Andrews Curves involve using attributes of samples as coefficients for Fourier series</span></span><br><span class="line"><span class="keyword">from</span> pandas.tools.plotting <span class="keyword">import</span> andrews_curves</span><br><span class="line">andrews_curves(df.drop(<span class="string">&quot;Id&quot;</span>, axis=<span class="number">1</span>), <span class="string">&quot;col&quot;</span>)</span><br><span class="line"><span class="comment">#都开始不太好理解了</span></span><br><span class="line"><span class="keyword">from</span> pandas.tools.plotting <span class="keyword">import</span> radviz</span><br><span class="line">radviz(df.drop(<span class="string">&quot;Id&quot;</span>, axis=<span class="number">1</span>), <span class="string">&quot;col&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="统计检验"><a href="#统计检验" class="headerlink" title="统计检验"></a>统计检验</h3><h3 id="数据清洗（参考kaggle课程Learn-Data-Cleaning-Tutorials-kaggle-com-）（这部分在实际中用到了即可以直接百度程序或者翻Kaggle课程里面代码）"><a href="#数据清洗（参考kaggle课程Learn-Data-Cleaning-Tutorials-kaggle-com-）（这部分在实际中用到了即可以直接百度程序或者翻Kaggle课程里面代码）" class="headerlink" title="数据清洗（参考kaggle课程Learn Data Cleaning Tutorials (kaggle.com)）（这部分在实际中用到了即可以直接百度程序或者翻Kaggle课程里面代码）"></a>数据清洗（参考kaggle课程<a target="_blank" rel="noopener" href="https://www.kaggle.com/learn/data-cleaning">Learn Data Cleaning Tutorials (kaggle.com)</a>）（这部分在实际中用到了即可以直接百度程序或者翻Kaggle课程里面代码）</h3><p>缺失值处理</p>
<p>面对缺失值我们首先要做出判断：<strong>Is this value missing because it wasn’t recorded or because it doesn’t exist?</strong>前者比如第一个特征是有无房子，第二个特征是房子尺寸，显然无房子的情况下第二个特征必然是missing value;另一方面则是我们常规意义上的缺失，一般通过插值来处理。</p>
<p>当然要把这些做的很完整当然时间很花力气的事，最简单的方法就是把那些具有NaN的列都删除掉，当然这肯定会损失一部分信息，稍微好一点的方法就是用<strong>fillna</strong>，具体方法可以参照函数本身的内容。</p>
<p>Scaling and Normalization</p>
<ul>
<li>in <strong>scaling</strong>, you’re changing the <em>range</em> of your data, while</li>
<li>in <strong>normalization</strong>, you’re changing the <em>shape of the distribution</em> of your data.</li>
<li>前者主要应用于SVM，KNN等（距离维度不同会带来很大的差异），后者就是把数据分布变成正态分布，一般采用的方法是Box-Cox变换，适用于那些对数据要求是正态分布的模型。</li>
</ul>
<p>对日期的处理</p>
<p>一般DataFrame这种变量类型都是Object，体现不出什么信息，这里可以通过函数pd.to_datetime来转换，使其变成有效的特征。</p>
<p>对数据输入不一致的处理</p>
<p>简单的说就是输入的数据可能本来是一个东西，但是由于字母大小不一致，或者多个空格，或者由于输入问题，或者表达问题， 导致一个单词有相似的表达方法，致统计出来的数据是多个,比如”Australia”和”australia”。通过调包fuzzywuzzy解决。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>处理missing data，outlier，Categorical Variable</p>
<h4 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对object数据填充最常用的(感觉会改变原始分布)，对numerical填充中位数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">handle_missing_values</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="comment"># Check for missing values in each column</span></span><br><span class="line">    missing_values = df.isnull().<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fill missing values according to data type</span></span><br><span class="line">    <span class="keyword">for</span> column, num_missing <span class="keyword">in</span> missing_values.items():</span><br><span class="line">        <span class="keyword">if</span> num_missing &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> df[column].dtype == <span class="string">&#x27;object&#x27;</span>:</span><br><span class="line">                <span class="comment"># Fill missing values in categorical columns with the most frequent value</span></span><br><span class="line">                df[column].fillna(df[column].mode()[<span class="number">0</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Fill missing values in numerical columns with the median value</span></span><br><span class="line">                df[column].fillna(df[column].median(), inplace=<span class="literal">True</span>)</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">handle_missing_values(train)</span><br></pre></td></tr></table></figure>
<p>另一种感觉也是较为常用的SimpleImputer，不过感觉做的有点过分，这是针对numeric，SimpleImputer可以参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/461017748">机器学习特征工程-缺失值填充：SimpleImputer - 知乎 (zhihu.com)</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">advanced_preprocess_numeric_columns</span>(<span class="params">df, method=<span class="string">&#x27;median&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Takes a Pandas DataFrame and performs advanced preprocessing on all numeric columns that need it.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Fills missing values using SimpleImputer, scales the data using StandardScaler, and removes outliers using the Z-score.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns the modified DataFrame.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Make a copy of the DataFrame to avoid modifying the original data</span></span><br><span class="line">    df = df.copy()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a list of all numeric columns</span></span><br><span class="line">    numeric_columns = df.select_dtypes(include=[<span class="string">&#x27;int64&#x27;</span>, <span class="string">&#x27;float64&#x27;</span>]).columns</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop through each numeric column</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> numeric_columns:</span><br><span class="line">        <span class="comment"># Check if the column has missing values or outliers</span></span><br><span class="line">        <span class="keyword">if</span> df[col].isnull().<span class="built_in">any</span>() <span class="keyword">or</span> df[col].between(df[col].quantile(<span class="number">.01</span>), df[col].quantile(<span class="number">.99</span>)).<span class="built_in">all</span>():</span><br><span class="line">            <span class="comment"># Fill missing values using SimpleImputer</span></span><br><span class="line">            imputer = SimpleImputer(strategy=method)</span><br><span class="line">            df[col] = imputer.fit_transform(df[col].values.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Scale the data using StandardScaler</span></span><br><span class="line">            scaler = StandardScaler()</span><br><span class="line">            df[col] = scaler.fit_transform(df[col].values.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Remove outliers using the Z-score</span></span><br><span class="line">            z_scores = stats.zscore(df[col])</span><br><span class="line">            df = df[np.<span class="built_in">abs</span>(z_scores) &lt; <span class="number">3</span>]</span><br><span class="line">            </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Advanced preprocessing steps completed for columns that needed it: filled missing values, scaled data, and removed outliers.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<p>补充一些missing_value的处理方法，灵感来自<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/kishan9044/story-of-titanic/notebook">Story of 🛳️ Titanic :) | Kaggle</a>(强烈安利这个，真的是入门最全的东西，最基础的Titanic都有很多东西可以总结)</p>
<p>核心观点就是Always make sure that the distribution of the data is same or close to similar before and after the imputation.</p>
<p>以Titanic的Age为例，第一种方法也是最常用的，就是把缺失值转换成均值或者中位数，但这显然会改变原始分布，毕竟某一个值会太突出</p>
<p>第二种方法，获得Age的均值与方差，然后用np.random.randint(low_range_train,high_range_train,size_train)生成数据(low_range_high=mean-std,high_range_train=mean+std)，但是这种方法仍然会改变原始分布</p>
<p>第三种方法，也是最常用的，直接在原始数据里随机抽样，效果是最好的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&quot;Age_imputed&quot;</span>][train[<span class="string">&quot;Age_imputed&quot;</span>].isna()] = random.sample(<span class="built_in">list</span>(train[<span class="string">&quot;Age&quot;</span>][~(train[<span class="string">&quot;Age&quot;</span>].isna())].values), <span class="built_in">len</span>(train[<span class="string">&quot;Age&quot;</span>][train[<span class="string">&quot;Age&quot;</span>].isna()]))</span><br></pre></td></tr></table></figure>
<p>这是针对分类变量，最常用的还是one-hot编码，但注意到one-hot编码会使得特征变得非常多，对一个n个取值特征进行one-hot编码，就会变成n列，所以计算量是非常大的，就不要一股脑全用上。还有所谓的Ordinal Encoding编码，就是不同特征编码12345，这就会涉及到一个特征取值是否可比较的问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">advanced_preprocess_categorical_columns</span>(<span class="params">df, encoding=<span class="string">&#x27;onehot&#x27;</span>, ordinal_columns=<span class="literal">None</span>, rare_threshold=<span class="number">10</span>, unknown_categories=<span class="string">&#x27;Other&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Takes a Pandas DataFrame and performs advanced preprocessing on all categorical columns.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Fills missing values with the mode, removes rare categories, and encodes the data using one-hot encoding or ordinal encoding.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        df: Pandas DataFrame</span></span><br><span class="line"><span class="string">        encoding: encoding method to use (&#x27;onehot&#x27; or &#x27;ordinal&#x27;)</span></span><br><span class="line"><span class="string">        ordinal_columns: dictionary of ordinal columns and their corresponding categories (e.g., &#123;&#x27;column_name&#x27;: [&#x27;low&#x27;, &#x27;medium&#x27;, &#x27;high&#x27;]&#125;)</span></span><br><span class="line"><span class="string">        rare_threshold: threshold for removing rare categories (default is 10)</span></span><br><span class="line"><span class="string">        unknown_categories: label for unknown categories (default is &#x27;Other&#x27;)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        modified DataFrame</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Make a copy of the DataFrame to avoid modifying the original data</span></span><br><span class="line">    df = df.copy()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a list of all categorical columns</span></span><br><span class="line">    categorical_columns = df.select_dtypes(include=[<span class="string">&#x27;object&#x27;</span>]).columns</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fill missing values with the mode</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> categorical_columns:</span><br><span class="line">        df[col] = df[col].fillna(df[col].mode()[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Remove rare categories</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> categorical_columns:</span><br><span class="line">        <span class="comment"># Get the counts of each category</span></span><br><span class="line">        counts = df[col].value_counts()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Remove categories with less than rare_threshold occurrences</span></span><br><span class="line">        df[col] = df[col].apply(<span class="keyword">lambda</span> x: x <span class="keyword">if</span> counts[x] &gt;= rare_threshold <span class="keyword">else</span> unknown_categories)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> encoding == <span class="string">&#x27;onehot&#x27;</span>:</span><br><span class="line">        <span class="comment"># Encode the data using one-hot encoding</span></span><br><span class="line">        df = pd.get_dummies(df, columns=categorical_columns, drop_first=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> encoding == <span class="string">&#x27;ordinal&#x27;</span>:</span><br><span class="line">        <span class="comment"># Check if ordinal_columns is provided</span></span><br><span class="line">        <span class="keyword">if</span> ordinal_columns <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;ordinal_columns must be provided when using ordinal encoding.&quot;</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Encode the data using ordinal encoding</span></span><br><span class="line">        <span class="keyword">for</span> col, categories <span class="keyword">in</span> ordinal_columns.items():</span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>, categories=categories, ordered=<span class="literal">True</span>)</span><br><span class="line">            df[col] = df[col].cat.codes</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid encoding method. Must be &#x27;onehot&#x27; or &#x27;ordinal&#x27;.&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Advanced preprocessing steps completed: filled missing values, removed rare categories, and encoded data.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="特征工程（参考Learn-Feature-Engineering-Tutorials-kaggle-com-）"><a href="#特征工程（参考Learn-Feature-Engineering-Tutorials-kaggle-com-）" class="headerlink" title="特征工程（参考Learn Feature Engineering Tutorials (kaggle.com)）"></a>特征工程（参考<a target="_blank" rel="noopener" href="https://www.kaggle.com/learn/feature-engineering">Learn Feature Engineering Tutorials (kaggle.com)</a>）</h2><h4 id="什么是特征工程"><a href="#什么是特征工程" class="headerlink" title="什么是特征工程"></a>什么是特征工程</h4><p>一般来说一个数据集可能有很多特征，你需要从中选择一部分重要的特征来做，这样的好处在于1提升模型性能2降低计算成本3提高可解释性。</p>
<h3 id="特征效用指标"><a href="#特征效用指标" class="headerlink" title="特征效用指标"></a>特征效用指标</h3><p>选择合适的特征，这里的选择是互信息，好处如下：</p>
<ul>
<li>easy to use and interpret,</li>
<li>computationally efficient,</li>
<li>theoretically well-founded,</li>
<li>resistant to overfitting, and,</li>
<li>able to detect any kind of relationship</li>
</ul>
<p>基本上互信息值越大，它对target的解释就越多，也就算是越好的特征。</p>
<p>但也有一些注意事项：</p>
<ul>
<li><p>MI can help you to understand the <em>relative potential</em> of a feature as a predictor of the target, considered by itself.</p>
</li>
<li><p>It’s possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI <em>can’t detect interactions</em> between features. It is a <strong>univariate</strong> metric.</p>
</li>
<li><p>The <em>actual</em> usefulness of a feature <em>depends on the model you use it with</em>. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn’t mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association.</p>
</li>
</ul>
<p>  具体函数在这里，算是一种简洁筛选特征的手段，当然也要注意到上面的事项。</p>
<ul>
<li>Scikit-learn has two mutual information metrics in its <code>feature_selection</code> module: one for real-valued targets (<code>mutual_info_regression</code>) and one for categorical targets (<code>mutual_info_classif</code>). Our target, <code>price</code>, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe.</li>
</ul>
<h3 id="发现新的特征"><a href="#发现新的特征" class="headerlink" title="发现新的特征"></a>发现新的特征</h3><p>发现新特征的Tips</p>
<ul>
<li>Understand the features. Refer to your dataset’s <em>data documentation</em>, if available.</li>
<li>Research the problem domain to acquire <strong>domain knowledge</strong>. If your problem is predicting house prices, do some research on real-estate for instance. Wikipedia can be a good starting point, but books and <a target="_blank" rel="noopener" href="https://scholar.google.com/">journal articles</a> will often have the best information.</li>
<li>Study previous work. <a target="_blank" rel="noopener" href="https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions">Solution write-ups</a> from past Kaggle competitions are a great resource.</li>
<li>Use data visualization. Visualization can reveal pathologies in the distribution of a feature or complicated relationships that could be simplified. Be sure to visualize your dataset as you work through the feature engineering process.</li>
</ul>
<h4 id="Mathematical-Transforms：对特征进行简单的加减乘除组合，一般来说通过数据可视化来调整组合"><a href="#Mathematical-Transforms：对特征进行简单的加减乘除组合，一般来说通过数据可视化来调整组合" class="headerlink" title="Mathematical Transforms：对特征进行简单的加减乘除组合，一般来说通过数据可视化来调整组合"></a>Mathematical Transforms：对特征进行简单的加减乘除组合，一般来说通过数据可视化来调整组合</h4><h3 id="Counts-Features-describing-the-presence-or-absence-of-something-often-come-in-sets-the-set-of-risk-factors-for-a-disease-say-You-can-aggregate-such-features-by-creating-a-count"><a href="#Counts-Features-describing-the-presence-or-absence-of-something-often-come-in-sets-the-set-of-risk-factors-for-a-disease-say-You-can-aggregate-such-features-by-creating-a-count" class="headerlink" title="Counts: Features describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a count."></a>Counts: Features describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a <strong>count</strong>.</h3><h3 id="Building-Up-and-Breaking-Down-Features-比如身份证前六位是地址什么的"><a href="#Building-Up-and-Breaking-Down-Features-比如身份证前六位是地址什么的" class="headerlink" title="Building-Up and Breaking-Down Features: 比如身份证前六位是地址什么的"></a>Building-Up and Breaking-Down Features: 比如身份证前六位是地址什么的</h3><ul>
<li>D numbers: <code>&#39;123-45-6789&#39;</code></li>
<li>Phone numbers: <code>&#39;(999) 555-0123&#39;</code></li>
<li>Street addresses: <code>&#39;8241 Kaggle Ln., Goose City, NV&#39;</code></li>
<li>Internet addresses: <code>&#39;http://www.kaggle.com</code></li>
<li>Product codes: <code>&#39;0 36000 29145 2&#39;</code></li>
<li>Dates and times: <code>&#39;Mon Sep 30 07:06:05 2013&#39;</code></li>
</ul>
<h3 id="Group-Transformation-Using-an-aggregation-function-a-group-transform-combines-two-features-a-categorical-feature-that-provides-the-grouping-and-another-feature-whose-values-you-wish-to-aggregate"><a href="#Group-Transformation-Using-an-aggregation-function-a-group-transform-combines-two-features-a-categorical-feature-that-provides-the-grouping-and-another-feature-whose-values-you-wish-to-aggregate" class="headerlink" title="Group Transformation: Using an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate"></a>Group Transformation: Using an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate</h3><p><strong>Tips on Creating Features</strong><br>It’s good to keep in mind your model’s own strengths and weaknesses when creating features. Here are some guidelines:</p>
<ul>
<li>Linear models learn sums and differences naturally, but can’t learn anything more complex.</li>
<li>Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.</li>
<li>Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.</li>
<li>Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.</li>
<li>Counts are especially helpful for tree models, since these models don’t have a natural way of aggregating information across many features at once.</li>
</ul>
<h3 id="将聚类标签作为特征"><a href="#将聚类标签作为特征" class="headerlink" title="将聚类标签作为特征"></a>将聚类标签作为特征</h3><p>The motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It’s a “divide and conquer” strategy.但是K的数目的确定倒是没有细说。</p>
<h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.</p>
<p>PCA的优点：</p>
<ul>
<li><strong>Dimensionality reduction</strong>: When your features are highly redundant (<em>multicollinear</em>, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.</li>
<li><strong>Anomaly detection</strong>: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.</li>
<li><strong>Noise reduction</strong>: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.</li>
<li><strong>Decorrelation</strong>: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.</li>
</ul>
<p><strong>PCA Best Practices</strong><br>There are a few things to keep in mind when applying PCA:</p>
<ul>
<li>PCA only works with numeric features, like continuous quantities or counts.</li>
<li>PCA is sensitive to scale. It’s good practice to standardize your data before applying PCA, unless you know you have good reason not to.</li>
<li>Consider removing or constraining outliers, since they can have an undue influence on the results.</li>
</ul>
<h3 id="Target-Encoding（针对分类变量）"><a href="#Target-Encoding（针对分类变量）" class="headerlink" title="Target Encoding（针对分类变量）"></a>Target Encoding（针对分类变量）</h3><h1 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h1><p>An encoding like this presents a couple of problems, however. First are <em>unknown categories</em>. Target encodings create a special risk of overfitting, which means they need to be trained on an independent “encoding” split. When you join the encoding to future splits, Pandas will fill in missing values for any categories not present in the encoding split. These missing values you would have to impute somehow.</p>
<p>Second are <em>rare categories</em>. When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate. In the <em>Automobiles</em> dataset, the <code>mercurcy</code> make only occurs once. The “mean” price we calculated is just the price of that one vehicle, which might not be very representative of any Mercuries we might see in the future. Target encoding rare categories can make overfitting more likely.</p>
<p>A solution to these problems is to add <strong>smoothing</strong>. The idea is to blend the <em>in-category</em> average with the <em>overall</em> average. Rare categories get less weight on their category average, while missing categories just get the overall average.</p>
<p><strong>Use Cases for Target Encoding</strong><br>Target encoding is great for:</p>
<ul>
<li><strong>High-cardinality features</strong>: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature’s most important property: its relationship with the target.</li>
<li><strong>Domain-motivated features</strong>: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature’s true informativeness.</li>
</ul>
<h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing for numerical data</span></span><br><span class="line">numerical_transformer = SimpleImputer(strategy=<span class="string">&#x27;constant&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing for categorical data</span></span><br><span class="line">categorical_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">&#x27;imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;most_frequent&#x27;</span>)),</span><br><span class="line">    (<span class="string">&#x27;onehot&#x27;</span>, OneHotEncoder(handle_unknown=<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bundle preprocessing for numerical and categorical data</span></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">&#x27;num&#x27;</span>, numerical_transformer, numerical_cols),</span><br><span class="line">        (<span class="string">&#x27;cat&#x27;</span>, categorical_transformer, categorical_cols)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line">model = RandomForestRegressor(n_estimators=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bundle preprocessing and modeling code in a pipeline</span></span><br><span class="line">my_pipeline = Pipeline(steps=[(<span class="string">&#x27;preprocessor&#x27;</span>, preprocessor),</span><br><span class="line">                              (<span class="string">&#x27;model&#x27;</span>, model)</span><br><span class="line">                             ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing of training data, fit model </span></span><br><span class="line">my_pipeline.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing of validation data, get predictions</span></span><br><span class="line">preds = my_pipeline.predict(X_valid)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the model</span></span><br><span class="line">score = mean_absolute_error(y_valid, preds)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE:&#x27;</span>, score)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/kaggle/" rel="tag"># kaggle</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/12/31/%E5%81%A5%E8%BA%AB/" rel="prev" title="健身">
                  <i class="fa fa-chevron-left"></i> 健身
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/01/03/leetcode/" rel="next" title="leetcode">
                  leetcode <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
